{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "wln0uIVB1ocr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing essential libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#loading the datasets\n",
        "\n",
        "(x_training_set, y_training_set), (x_testing_set, y_testing_set) = fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "id": "NkjrbUq91qOX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data for cross validation\n",
        "\n",
        "x_validation_set = x_training_set[50000:]\n",
        "y_validation_set = y_training_set[50000:]     # validation set has 10000 data\n",
        "\n",
        "x_training_set = x_training_set[:50000]\n",
        "y_training_set = y_training_set[:50000]\n",
        "\n",
        "\n",
        "#vactorising the data\n",
        "\n",
        "x_training_set = x_training_set.reshape(x_training_set.shape[0], 784)  #28x28 pixels = 784\n",
        "x_testing_set = x_testing_set.reshape(x_testing_set.shape[0], 784)\n",
        "x_validation_set = x_validation_set.reshape(x_validation_set.shape[0], 784)\n",
        "\n",
        "#normalising the data\n",
        "\n",
        "x_train = x_training_set/255  # since, pixel range from 0 to 255\n",
        "x_test = x_testing_set/255\n",
        "x_valid = x_validation_set/255\n",
        "\n",
        "#one hot encoding for labels to represent categorical variables as numerical values\n",
        "\n",
        "y_train = to_categorical(y_training_set)\n",
        "y_test = to_categorical(y_testing_set)\n",
        "y_valid = to_categorical(y_validation_set)\n",
        "\n",
        "\n",
        "# some useful functions\n",
        "\n",
        "#for hidden layer\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def gradient_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "#for output layer\n",
        "def softmax(x):\n",
        "    exponents = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exponents / np.sum(exponents, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "iR0SSbCL1zAH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class definition\n",
        "\n",
        "class Network:\n",
        "  def __init__(self, neuron_sizes):\n",
        "    self.total_layers = len(neuron_sizes)\n",
        "\n",
        "    #initialising the weights and biases\n",
        "    #After initialising storing weights and biases in separate lists\n",
        "    self.Weights = [np.random.randn(neuron_sizes[i], neuron_sizes[i+1])*0.05 for i in range(self.total_layers-1)]\n",
        "    self.biases = [np.random.randn(1, neuron_sizes[i+1])*0.05 for i in range(self.total_layers-1)]\n",
        "\n",
        "\n",
        "  #defining function for forward propagation\n",
        "  def forward_prop(self, X):\n",
        "    self.pre_activations_A = [None]*(self.total_layers)  #list to store the pre-activations\n",
        "    self.activations_H = [X]  #list to store the activations\n",
        "\n",
        "    for i in range(self.total_layers-1):\n",
        "      self.pre_activations_A[i+1] = np.dot(self.activations_H[i], self.Weights[i]) + self.biases[i]\n",
        "\n",
        "      if i == self.total_layers-2:  #for output layer: activation function = softmax\n",
        "        h = softmax(self.pre_activations_A[i+1])\n",
        "        self.activations_H.append(h)\n",
        "\n",
        "      else:  #for hidden layers: activation function = sigmoid\n",
        "        h = sigmoid(self.pre_activations_A[i+1])\n",
        "        self.activations_H.append(h)\n",
        "    return self.activations_H[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "W0nWkMsz2zWI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing the parameters\n",
        "\n",
        "#these all are hyper-parameters we can tune then to avoid overfitting\n",
        "neuron_sizes = [784, 64, 64, 10]   #input size, hidden layers, output size\n",
        "learning_rate_eta = 0.01\n",
        "total_epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "#initialization of the neural network by making an object of class network\n",
        "my_model = Network(neuron_sizes)"
      ],
      "metadata": {
        "id": "mVFaLPsv4BJc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = my_model.forward_prop(x_train)"
      ],
      "metadata": {
        "id": "1IBXz1nu439U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ-9IOXu8U-J",
        "outputId": "2de25f95-8393-41a7-9947-6ed05d9e83d1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.08307613 0.07371361 0.123548   ... 0.0983791  0.08373833 0.10773881]\n",
            " [0.08252284 0.07437419 0.12525714 ... 0.0977909  0.084644   0.10735657]\n",
            " [0.08319421 0.07368614 0.12497619 ... 0.098262   0.08428977 0.10748327]\n",
            " ...\n",
            " [0.08284599 0.0736776  0.12514944 ... 0.09824595 0.08437049 0.10729375]\n",
            " [0.08289171 0.07366876 0.12487186 ... 0.09831357 0.08428044 0.107841  ]\n",
            " [0.08322459 0.07360706 0.124295   ... 0.09798421 0.0844034  0.10730283]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(round(sum(pred[0]),6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Y092Gp8MJT",
        "outputId": "28097cff-22a8-44e6-93a1-4a1c679dd34e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-bguYea8OTD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}