{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52bc44a694104817930fe014eadd0449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8ca24f658de4ee5a2f50add4ffbd0fa",
              "IPY_MODEL_303bb3d6e24f472088bcdcecb8783624"
            ],
            "layout": "IPY_MODEL_48db94c8ba4746f1a7ae320638e1c9bb"
          }
        },
        "e8ca24f658de4ee5a2f50add4ffbd0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00a83ab1bfa49ca804fb1f173a47ea9",
            "placeholder": "​",
            "style": "IPY_MODEL_24bc7713031a437bbd4ce98ab12e0a38",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "303bb3d6e24f472088bcdcecb8783624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640969c4ef45467ea2515cfc0cfd1f82",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c0901a2cf434372935553ec75e4c7b4",
            "value": 1
          }
        },
        "48db94c8ba4746f1a7ae320638e1c9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00a83ab1bfa49ca804fb1f173a47ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24bc7713031a437bbd4ce98ab12e0a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "640969c4ef45467ea2515cfc0cfd1f82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0901a2cf434372935553ec75e4c7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08e3f886ce4343be97670731489d05f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf433c7bf4fe4d37835fd6dee6b90229",
              "IPY_MODEL_42e56091ab9d4ee7bcf087db8748af91"
            ],
            "layout": "IPY_MODEL_530638ffc2784ac095279d12ca43bb4f"
          }
        },
        "cf433c7bf4fe4d37835fd6dee6b90229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a154224b524d2981bd5e22484bd0f6",
            "placeholder": "​",
            "style": "IPY_MODEL_44259004640a44878b1e0825f512f05e",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "42e56091ab9d4ee7bcf087db8748af91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1faa7c54a6224dca9448837da3de8b9e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6bfd8da7da7411b87e248ae04e1002d",
            "value": 1
          }
        },
        "530638ffc2784ac095279d12ca43bb4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a154224b524d2981bd5e22484bd0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44259004640a44878b1e0825f512f05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1faa7c54a6224dca9448837da3de8b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6bfd8da7da7411b87e248ae04e1002d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba378142800f4a1a81ae1fbcae7bb238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_008d7366a4ef4549b12d5a593b0a613a",
              "IPY_MODEL_f19854e066cf4198b08663e5896cac0a"
            ],
            "layout": "IPY_MODEL_4d50df139f6b4be09052cb24f3e83d00"
          }
        },
        "008d7366a4ef4549b12d5a593b0a613a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61182ee22fa2402b8d55193b15178b79",
            "placeholder": "​",
            "style": "IPY_MODEL_7099f82f4b014bf2a704648a8b7fde62",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "f19854e066cf4198b08663e5896cac0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b2d75bb315248d8bf830c021a372ae1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_999b569bcb50465fab19ad0911b4af64",
            "value": 1
          }
        },
        "4d50df139f6b4be09052cb24f3e83d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61182ee22fa2402b8d55193b15178b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7099f82f4b014bf2a704648a8b7fde62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b2d75bb315248d8bf830c021a372ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "999b569bcb50465fab19ad0911b4af64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "zMZF-WaA_l8Q",
        "outputId": "2b1d51ea-cf39-4965-ebed-0da90d29300c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#function definition to plot the one image from different classes\\n\\ndef plot_img(images, labels, classes):\\n  image_list = [] #list to store one image from each class\\n  class_num = len(classes)\\n\\n  for i in range(class_num):\\n    indx = np.where(labels == i)[0][0]\\n    image_list.append(images[indx])\\n\\n  #plotting the images\\n\\n  plt.figure(figsize = (10,10))\\n  for i in range(class_num):\\n    plt.subplot(5,5,i+1)\\n    plt.xticks([])\\n    plt.yticks([])\\n    plt.grid(False)\\n    plt.imshow(image_list[i], cmap=plt.cm.binary)\\n    plt.xlabel(classes[i])\\n  plt.show()\\n\\n\\n#calling the function\\n\\nplot_img(x_training_set, y_training_set, classes)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#importing essential libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#loading the datasets\n",
        "\n",
        "(x_training_set, y_training_set), (x_testing_set, y_testing_set) = mnist.load_data()\n",
        "\n",
        "#storing different classes in a list\n",
        "\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "'''\n",
        "#function definition to plot the one image from different classes\n",
        "\n",
        "def plot_img(images, labels, classes):\n",
        "  image_list = [] #list to store one image from each class\n",
        "  class_num = len(classes)\n",
        "\n",
        "  for i in range(class_num):\n",
        "    indx = np.where(labels == i)[0][0]\n",
        "    image_list.append(images[indx])\n",
        "\n",
        "  #plotting the images\n",
        "\n",
        "  plt.figure(figsize = (10,10))\n",
        "  for i in range(class_num):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image_list[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(classes[i])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "#calling the function\n",
        "\n",
        "plot_img(x_training_set, y_training_set, classes)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data for cross validation\n",
        "\n",
        "x_validation_set = x_training_set[50000:]\n",
        "y_validation_set = y_training_set[50000:]     # validation set has 10000 data\n",
        "\n",
        "x_training_set = x_training_set[:50000]\n",
        "y_training_set = y_training_set[:50000]\n",
        "\n",
        "\n",
        "#vactorising the data\n",
        "\n",
        "x_training_set = x_training_set.reshape(x_training_set.shape[0], 784)  #28x28=784\n",
        "x_testing_set = x_testing_set.reshape(x_testing_set.shape[0], 784)\n",
        "x_validation_set = x_validation_set.reshape(x_validation_set.shape[0], 784)\n",
        "\n",
        "#normalising the data\n",
        "\n",
        "x_train = x_training_set/255  # since, pixel range from 0 to 255\n",
        "x_test = x_testing_set/255\n",
        "x_valid = x_validation_set/255\n",
        "\n",
        "#one hot encoding for labels to represent categorical variables as numerical values\n",
        "\n",
        "y_train = to_categorical(y_training_set)\n",
        "y_test = to_categorical(y_testing_set)\n",
        "y_valid = to_categorical(y_validation_set)\n",
        "\n",
        "\n",
        "#default_x_train = x_train\n",
        "#default_y_train = y_train\n",
        "\n",
        "# some useful functions\n",
        "\n",
        "#for hidden layer\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return (x>0)*(x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def gradient_sigmoid(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def gradient_tanh(x):\n",
        "  return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def gradient_relu(x):\n",
        "  return np.where(x > 0, 1, 0)\n",
        "\n",
        "#for output layer\n",
        "def softmax(x):\n",
        "  exponents = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "  return exponents / np.sum(exponents, axis=1, keepdims=True)\n",
        "\n",
        "'''\n",
        "#loss function(cross-entropy)\n",
        "def loss_func(x,y):\n",
        "  L = -np.mean(np.sum(x * np.log(y), axis=1))\n",
        "  return L'''\n",
        "\n",
        "# loss function (cross-entropy)\n",
        "def loss_func(x, y):\n",
        "    epsilon = 1e-10  # small epsilon value to avoid log overflow\n",
        "    clipped_y = np.clip(y, epsilon, 1 - epsilon)  # clip predicted probabilities\n",
        "    L = -np.mean(np.sum(x * np.log(clipped_y), axis=1))\n",
        "    return L\n",
        "\n",
        "\n",
        "#function to choose the activation functions\n",
        "\n",
        "def choose_activation(x, activation_function):\n",
        "  if activation_function == 'sigmoid':\n",
        "    return sigmoid(x)\n",
        "\n",
        "  elif activation_function == 'tanh':\n",
        "    return tanh(x)\n",
        "\n",
        "  elif activation_function == 'relu':\n",
        "    return relu(x)\n",
        "\n",
        "\n",
        "#function for derivatives\n",
        "\n",
        "def activation_derivative(x, activation_function):\n",
        "    if activation_function == 'sigmoid':\n",
        "        return gradient_sigmoid(x)\n",
        "    elif activation_function == 'relu':\n",
        "        return gradient_relu(x)\n",
        "    elif activation_function == 'tanh':\n",
        "        return gradient_tanh(x)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid activation function. Please choose from 'sigmoid', 'relu', or 'tanh'.\")"
      ],
      "metadata": {
        "id": "qJSrVkMpNbXo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class definition\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, neuron_sizes, weight_initialiser, activation_function, momentum, beta1, beta2):\n",
        "\n",
        "        self.train_loss_sgd = []\n",
        "        self.train_loss_momentum = []\n",
        "\n",
        "\n",
        "        self.total_layers = len(neuron_sizes)\n",
        "        self.momentum = momentum\n",
        "        #self.weight_initialiser = weight_initialiser\n",
        "        self.activation_function = activation_function\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "        # Initializing the weights and biases\n",
        "        # After initializing, store weights and biases in separate lists\n",
        "        if weight_initialiser == 'xavier':\n",
        "            if activation_function != 'relu':\n",
        "                self.Weights = [-1/np.sqrt(neuron_sizes[i])+np.random.randn(neuron_sizes[i], neuron_sizes[i+1])*2/np.sqrt(neuron_sizes[i]) for i in range(self.total_layers-1)]\n",
        "                self.biases = [-1/np.sqrt(neuron_sizes[i])+np.random.randn(1, neuron_sizes[i+1])*2/np.sqrt(neuron_sizes[i]) for i in range(self.total_layers-1)]\n",
        "            else:\n",
        "                self.Weights = [np.random.randn(neuron_sizes[i], neuron_sizes[i+1])*(np.sqrt(2/(neuron_sizes[i]))) for i in range(self.total_layers-1)]\n",
        "                self.biases = [np.random.randn(1, neuron_sizes[i+1])*(np.sqrt(2/(neuron_sizes[i]))) for i in range(self.total_layers-1)]\n",
        "        else:\n",
        "            self.Weights = [np.random.randn(neuron_sizes[i], neuron_sizes[i+1]) for i in range(self.total_layers-1)]\n",
        "            self.biases = [np.random.randn(1, neuron_sizes[i+1])*0.05 for i in range(self.total_layers-1)]\n",
        "\n",
        "        # Initializing momentum weights and biases\n",
        "        self.Weights_moment = [np.zeros_like(x) for x in self.Weights]\n",
        "        self.biases_moment = [np.zeros_like(x) for x in self.biases]\n",
        "\n",
        "        # Initializing Nesterov momentum adjusted weights and biases\n",
        "        self.momentum_adjusted_Weights = None\n",
        "        self.momentum_adjusted_biases = None\n",
        "\n",
        "\n",
        "        # Initializing for adam optimizer\n",
        "        self.Weights_moment_adam1 = [np.zeros_like(x) for x in self.Weights]\n",
        "        self.biases_moment_adam1 = [np.zeros_like(x) for x in self.biases]\n",
        "        self.Weights_moment_adam2 = [np.zeros_like(x) for x in self.Weights]  #rmsprop\n",
        "        self.biases_moment_adam2 = [np.zeros_like(x) for x in self.biases]   #rmsprop\n",
        "\n",
        "\n",
        "        # Initializing for nadam optimizer\n",
        "        self.Weights_moment_nadam1 = [np.zeros_like(x) for x in self.Weights]\n",
        "        self.biases_moment_nadam1 = [np.zeros_like(x) for x in self.biases]\n",
        "        self.Weights_moment_nadam2 = [np.zeros_like(x) for x in self.Weights]\n",
        "        self.biases_moment_nadam2 = [np.zeros_like(x) for x in self.biases]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Defining function for forward propagation\n",
        "    def forward_prop(self, X):\n",
        "        self.pre_activations_A = [None]*(self.total_layers)  # List to store the pre-activations\n",
        "        self.activations_H = [X]  # List to store the activations\n",
        "\n",
        "        for i in range(self.total_layers-1):\n",
        "            self.pre_activations_A[i+1] = np.dot(self.activations_H[i], self.Weights[i]) + self.biases[i]\n",
        "            if i == self.total_layers-2:  # For output layer: activation function = softmax\n",
        "                h = softmax(self.pre_activations_A[i+1])\n",
        "                self.activations_H.append(h)\n",
        "            else:  # For hidden layers: activation function = sigmoid\n",
        "                h = choose_activation(self.pre_activations_A[i+1], self.activation_function)\n",
        "                #h = sigmoid(self.pre_activations_A[i+1])\n",
        "                self.activations_H.append(h)\n",
        "        return self.activations_H[-1]\n",
        "\n",
        "    #defining the training method for stochastic gradient method\n",
        "    def train_sgd(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "        for epoch in range(total_epochs):\n",
        "            loss_epo = 0\n",
        "            accuracy = 0\n",
        "            for i in range(0, x_train.shape[0], batch_size):\n",
        "                # Forward pass\n",
        "                Xbatch = x_train[i:i+batch_size]\n",
        "                Ybatch = y_train[i:i+batch_size]\n",
        "                y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "                #calculate loss\n",
        "                loss = loss_func(Ybatch, y_predicted)\n",
        "                loss_epo = loss_epo+loss\n",
        "\n",
        "                #calculate accuracy\n",
        "                acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "                accuracy = accuracy + acc\n",
        "\n",
        "                # Applying backpropagation algorithm\n",
        "                loss_gradient = y_predicted - Ybatch\n",
        "                for j in range(self.total_layers - 1, 0, -1):\n",
        "                    gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "                    gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "                    if j > 1:\n",
        "                        derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "                        loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T) * derivative_activation\n",
        "                        #loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T) * (self.activations_H[j-1] * (1 - self.activations_H[j-1]))\n",
        "\n",
        "                    #updation of parameters\n",
        "                    self.Weights[j-1] = self.Weights[j-1] - learning_rate_eta * gradient_W\n",
        "                    self.biases[j-1] = self.biases[j-1] - learning_rate_eta * gradient_b\n",
        "\n",
        "\n",
        "            #computing average train accuracy\n",
        "            training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "            wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "            #computing average epoch loss\n",
        "            loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "            wandb.log({'train-loss':loss_epo})\n",
        "            #self.train_loss_sgd.append(loss_epo)\n",
        "            '''\n",
        "            #computing training loss\n",
        "            train_loss = loss_func(y_train, y_predicted)\n",
        "            print(f'Epoch Number {epoch+1}, training loss: {valid_accuracy:.4f}')'''\n",
        "\n",
        "            #computing accuracy on validation set\n",
        "            y_valid_predicted = self.forward_prop(x_valid)\n",
        "            valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "            print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "            wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "            wandb.log({'epoch':epoch+1})\n",
        "\n",
        "            #computing validation loss\n",
        "            val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "            print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "            wandb.log({'val-loss':val_loss})\n",
        "\n",
        "\n",
        "        #checking the efficiency of the model by passing test set\n",
        "        y_test_predicted = self.forward_prop(x_test)\n",
        "        test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "        print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    # Defining the momentum-based gradient descent training method\n",
        "    def train_momentum(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "        for epoch in range(total_epochs):\n",
        "            loss_epo = 0\n",
        "            accuracy = 0\n",
        "            for i in range(0, x_train.shape[0], batch_size):\n",
        "\n",
        "                #performing the forward pass\n",
        "                Xbatch = x_train[i:i+batch_size]\n",
        "                Ybatch = y_train[i:i+batch_size]\n",
        "                y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "                #calculate loss\n",
        "                loss = loss_func(Ybatch, y_predicted)\n",
        "                loss_epo = loss_epo+loss\n",
        "\n",
        "                #calculate accuracy\n",
        "                acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "                accuracy = accuracy + acc\n",
        "\n",
        "                #performing the back-propagation\n",
        "                loss_gradient = y_predicted - Ybatch\n",
        "                for j in range(self.total_layers - 1, 0, -1):\n",
        "                    gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "                    gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "                    if j > 1:\n",
        "                        derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "                        loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T) * derivative_activation\n",
        "                        #loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T) * (self.activations_H[j-1] * (1 - self.activations_H[j-1]))\n",
        "\n",
        "                    #updation of momentum\n",
        "                    self.Weights_moment[j-1] = self.momentum * self.Weights_moment[j-1] + learning_rate_eta * gradient_W\n",
        "                    self.biases_moment[j-1] = self.momentum * self.biases_moment[j-1] + learning_rate_eta * gradient_b\n",
        "\n",
        "                    #updation of parameters\n",
        "                    self.Weights[j-1] = self.Weights[j-1] - self.Weights_moment[j-1]\n",
        "                    self.biases[j-1] = self.biases[j-1] - self.biases_moment[j-1]\n",
        "\n",
        "            #computing average train accuracy\n",
        "            training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "            wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "            #computing average epoch(training loss) loss\n",
        "            loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "            wandb.log({'train-loss':loss_epo})\n",
        "            #self.train_loss_sgd.append(loss_epo)\n",
        "\n",
        "            #computing accuracy on validation set\n",
        "            y_valid_predicted = self.forward_prop(x_valid)\n",
        "            valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "            print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "            wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "            wandb.log({'epoch':epoch+1})\n",
        "\n",
        "            #computing validation loss\n",
        "            val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "            print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "            wandb.log({'val-loss':val_loss})\n",
        "\n",
        "        #checking the efficiency of the model by passing test set\n",
        "        y_test_predicted = self.forward_prop(x_test)\n",
        "        test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "        print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    #defining the training method for nesterov accelerated gradient descent method\n",
        "    def train_nag(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "        for epoch in range(total_epochs):\n",
        "            loss_epo = 0\n",
        "            accuracy = 0\n",
        "            for i in range(0, x_train.shape[0], batch_size):\n",
        "                #performing forward pass\n",
        "                Xbatch = x_train[i:i+batch_size]\n",
        "                Ybatch = y_train[i:i+batch_size]\n",
        "\n",
        "                # Nesterov accelerated gradient descent: lookahead\n",
        "                '''\n",
        "                self.momentum_adjusted_Weights = [self.Weights[j-1] - self.momentum * self.Weights_moment[j-1] for j in range(self.total_layers - 1, 0, -1)]\n",
        "                self.momentum_adjusted_biases = [self.biases[j-1] - self.momentum * self.biases_moment[j-1] for j in range(self.total_layers - 1, 0, -1)]'''\n",
        "\n",
        "                self.momentum_adjusted_Weights = [self.Weights[j] - self.momentum * self.Weights_moment[j] for j in range(self.total_layers-1)]\n",
        "                self.momentum_adjusted_biases = [self.biases[j] - self.momentum * self.biases_moment[j] for j in range(self.total_layers-1)]\n",
        "                '''\n",
        "                print(self.momentum_adjusted_Weights[-1].shape)\n",
        "                print(self.momentum_adjusted_Weights[-2].shape)\n",
        "                print(self.momentum_adjusted_Weights[-3].shape)'''\n",
        "\n",
        "                y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "\n",
        "                #calculate loss\n",
        "                loss = loss_func(Ybatch, y_predicted)\n",
        "                loss_epo = loss_epo+loss\n",
        "\n",
        "                #calculate accuracy\n",
        "                acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "                accuracy = accuracy + acc\n",
        "\n",
        "\n",
        "                #performing back propagation\n",
        "                loss_gradient = y_predicted - Ybatch\n",
        "                for j in range(self.total_layers - 1, 0, -1):\n",
        "                    gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "                    gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "                    if j > 1:\n",
        "                        #derivative_activation = 1 - self.activations_H[j-1] ** 2\n",
        "                        derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "                        loss_gradient = np.dot(loss_gradient, self.momentum_adjusted_Weights[j-1].T) * derivative_activation\n",
        "\n",
        "                        #loss_gradient = np.dot(loss_gradient, self.momentum_adjusted_Weights[j-1].T) * (self.activations_H[j-1] * (1 - self.activations_H[j-1]))\n",
        "\n",
        "                    self.Weights[j-1] = self.momentum_adjusted_Weights[j-1] - learning_rate_eta * gradient_W\n",
        "                    self.biases[j-1] = self.momentum_adjusted_biases[j-1] - learning_rate_eta * gradient_b\n",
        "\n",
        "\n",
        "            #computing average train accuracy\n",
        "            training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "            wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "            #computing average epoch(training loss) loss\n",
        "            loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "            print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "            wandb.log({'train-loss':loss_epo})\n",
        "            #self.train_loss_sgd.append(loss_epo)\n",
        "\n",
        "            #computing accuracy on validation set\n",
        "            y_valid_predicted = self.forward_prop(x_valid)\n",
        "            valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "            print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "            wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "            wandb.log({'epoch':epoch+1})\n",
        "\n",
        "            #computing validation loss\n",
        "            val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "            print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "            wandb.log({'val-loss':val_loss})\n",
        "\n",
        "        #checking the efficiency of the model by passing test set\n",
        "        y_test_predicted = self.forward_prop(x_test)\n",
        "        test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "        print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    def train_adam(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "      for epoch in range(total_epochs):\n",
        "        loss_epo = 0\n",
        "        accuracy = 0\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "          #performing forward pass\n",
        "          Xbatch = x_train[i:i+batch_size]\n",
        "          Ybatch = y_train[i:i+batch_size]\n",
        "          y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "          #calculate loss\n",
        "          loss = loss_func(Ybatch, y_predicted)\n",
        "          loss_epo = loss_epo + loss\n",
        "\n",
        "          #calculate accuracy\n",
        "          acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "          accuracy = accuracy + acc\n",
        "\n",
        "          #backpropagation\n",
        "          loss_gradient = y_predicted-Ybatch\n",
        "          for j in range(self.total_layers-1, 0, -1):\n",
        "            gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "            gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "\n",
        "            if j > 1:\n",
        "              derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "              loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T)*derivative_activation\n",
        "\n",
        "            #compute 1st momentum term\n",
        "            self.Weights_moment_adam1[j-1] = self.beta1 * self.Weights_moment_adam1[j-1] + (1-self.beta1) * gradient_W\n",
        "            self.biases_moment_adam1[j-1] = self.beta1 * self.biases_moment_adam1[j-1] + (1-self.beta1) * gradient_b\n",
        "\n",
        "            #compute 2nd moment term\n",
        "            self.Weights_moment_adam2[j-1] = self.beta2 * self.Weights_moment_adam2[j-1] + (1-self.beta2) * np.square(gradient_W)\n",
        "            self.biases_moment_adam2[j-1] = self.beta2 * self.biases_moment_adam2[j-1] + (1-self.beta2) * np.square(gradient_b)\n",
        "\n",
        "            #corrected terms in 1st moment\n",
        "            corrected_weight_adam1 = self.Weights_moment_adam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "            corrected_bias_adam1 = self.biases_moment_adam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "\n",
        "            #corrected terms in 2nd moment\n",
        "            corrected_weight_adam2 = self.Weights_moment_adam2[j-1] / (1-self.beta2 ** (epoch+1))\n",
        "            corrected_bias_adam2 = self.biases_moment_adam2[j-1] / (1-self.beta2 ** (epoch+1))\n",
        "\n",
        "            #updating weights and biases\n",
        "            self.Weights[j-1] = self.Weights[j-1] - learning_rate_eta * corrected_weight_adam1 / (np.sqrt(corrected_weight_adam2)+self.epsilon)\n",
        "            self.biases[j-1] = self.biases[j-1] - learning_rate_eta * corrected_bias_adam1 / (np.sqrt(corrected_bias_adam2)+self.epsilon)\n",
        "\n",
        "\n",
        "        #computing average train accuracy\n",
        "        training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "        wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "\n",
        "        #computing average epoch(training loss) loss\n",
        "        loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "        wandb.log({'train-loss':loss_epo})\n",
        "\n",
        "\n",
        "        #computing accuracy on validation set\n",
        "        y_valid_predicted = self.forward_prop(x_valid)\n",
        "        valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "        print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "        wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "        wandb.log({'epoch':epoch+1})\n",
        "\n",
        "        #computing validation loss\n",
        "        val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "        print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "        wandb.log({'val-loss':val_loss})\n",
        "\n",
        "      #checking the efficiency of the model by passing test set\n",
        "      y_test_predicted = self.forward_prop(x_test)\n",
        "      #return y_test_predicted\n",
        "\n",
        "      test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "      print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    def train_rmsprop(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "      for epoch in range(total_epochs):\n",
        "        loss_epo = 0\n",
        "        accuracy = 0\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "          Xbatch = x_train[i:i+batch_size]\n",
        "          Ybatch = y_train[i:i+batch_size]\n",
        "          y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "          #calculate loss\n",
        "          loss = loss_func(Ybatch, y_predicted)\n",
        "          loss_epo = loss_epo + loss\n",
        "\n",
        "          #calculate accuracy\n",
        "          acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "          accuracy = accuracy + acc\n",
        "\n",
        "          #backpropagation\n",
        "          loss_gradient = y_predicted-Ybatch\n",
        "          for j in range(self.total_layers-1, 0, -1):\n",
        "            gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "            gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "\n",
        "            if j > 1:\n",
        "              derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "              loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T)*derivative_activation\n",
        "            '''\n",
        "            #compute 1st momentum term\n",
        "            self.Weights_moment_adam1[j-1] = self.beta1 * self.Weights_moment_adam1[j-1] + (1-self.beta1) * gradient_W\n",
        "            self.biases_moment_adam1[j-1] = self.beta1 * self.biases_moment_adam1[j-1] + (1-self.beta1) * gradient_b'''\n",
        "\n",
        "            #compute moment term\n",
        "            self.Weights_moment_adam2[j-1] = self.beta2 * self.Weights_moment_adam2[j-1] + (1-self.beta2) * np.square(gradient_W)\n",
        "            self.biases_moment_adam2[j-1] = self.beta2 * self.biases_moment_adam2[j-1] + (1-self.beta2) * np.square(gradient_b)\n",
        "            '''\n",
        "            #corrected terms in 1st moment\n",
        "            corrected_weight_adam1 = self.Weights_moment_adam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "            corrected_bias_adam1 = self.biases_moment_adam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "\n",
        "            #corrected terms in 2nd moment\n",
        "            corrected_weight_adam2 = self.Weights_moment_adam2[j-1] / (1-self.beta2 ** (epoch+1))\n",
        "            corrected_bias_adam2 = self.biases_moment_adam2[j-1] / (1-self.beta2 ** (epoch+1))'''\n",
        "\n",
        "            #updating weights and biases\n",
        "            '''\n",
        "            self.Weights[j-1] = self.Weights[j-1] - learning_rate_eta * corrected_weight_adam1 / (np.sqrt(corrected_weight_adam2)+self.epsilon)\n",
        "            self.biases[j-1] = self.biases[j-1] - learning_rate_eta * corrected_bias_adam1 / (np.sqrt(corrected_bias_adam2)+self.epsilon)'''\n",
        "\n",
        "            self.Weights[j-1] = self.Weights[j-1] - learning_rate_eta * gradient_W / (np.sqrt(self.Weights_moment_adam2[j-1]) + self.epsilon)\n",
        "            self.biases[j-1] = self.biases[j-1] - learning_rate_eta * gradient_b / (np.sqrt(self.biases_moment_adam2[j-1]) + self.epsilon)\n",
        "\n",
        "        #computing average train accuracy\n",
        "        training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "        wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "        #computing average epoch(training loss) loss\n",
        "        loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "        wandb.log({'train-loss':loss_epo})\n",
        "        #self.train_loss_sgd.append(loss_epo)\n",
        "\n",
        "        #computing accuracy on validation set\n",
        "        y_valid_predicted = self.forward_prop(x_valid)\n",
        "        valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "        print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "        wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "        wandb.log({'epoch':epoch+1})\n",
        "\n",
        "        #computing validation loss\n",
        "        val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "        print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "        wandb.log({'val-loss':val_loss})\n",
        "\n",
        "      #checking the efficiency of the model by passing test set\n",
        "      y_test_predicted = self.forward_prop(x_test)\n",
        "      test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "      print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    def train_nadam(self, x_train, y_train, learning_rate_eta, total_epochs, batch_size):\n",
        "      for epoch in range(total_epochs):\n",
        "        loss_epo = 0\n",
        "        accuracy = 0\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "          Xbatch = x_train[i:i+batch_size]\n",
        "          Ybatch = y_train[i:i+batch_size]\n",
        "          y_predicted = self.forward_prop(Xbatch)\n",
        "\n",
        "          #calculate loss\n",
        "          loss = loss_func(Ybatch, y_predicted)\n",
        "          loss_epo = loss_epo + loss\n",
        "\n",
        "          #calculate accuracy\n",
        "          acc = accuracy_score(np.argmax(y_predicted, axis=1), np.argmax(Ybatch, axis=1))\n",
        "          accuracy = accuracy + acc\n",
        "\n",
        "          #backpropagation\n",
        "          loss_gradient = y_predicted-Ybatch\n",
        "          for j in range(self.total_layers-1, 0, -1):\n",
        "            gradient_W = np.dot(self.activations_H[j-1].T, loss_gradient)\n",
        "            gradient_b = np.sum(loss_gradient, axis=0, keepdims=True)\n",
        "\n",
        "            if j > 1:\n",
        "              derivative_activation = activation_derivative(self.pre_activations_A[j-1], self.activation_function)\n",
        "              loss_gradient = np.dot(loss_gradient, self.Weights[j-1].T)*derivative_activation\n",
        "\n",
        "            #compute 1st momentum term\n",
        "            self.Weights_moment_nadam1[j-1] = self.beta1 * self.Weights_moment_nadam1[j-1] + (1-self.beta1) * gradient_W\n",
        "            self.biases_moment_nadam1[j-1] = self.beta1 * self.biases_moment_nadam1[j-1] + (1-self.beta1) * gradient_b\n",
        "\n",
        "            #compute 2nd moment term\n",
        "            self.Weights_moment_nadam2[j-1] = self.beta2 * self.Weights_moment_nadam2[j-1] + (1-self.beta2) * np.square(gradient_W)\n",
        "            self.biases_moment_nadam2[j-1] = self.beta2 * self.biases_moment_nadam2[j-1] + (1-self.beta2) * np.square(gradient_b)\n",
        "\n",
        "            #corrected terms in 1st moment\n",
        "            corrected_weight_nadam1 = self.Weights_moment_nadam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "            corrected_bias_nadam1 = self.biases_moment_nadam1[j-1] / (1-self.beta1 ** (epoch+1))\n",
        "\n",
        "            #corrected terms in 2nd moment\n",
        "            corrected_weight_nadam2 = self.Weights_moment_nadam2[j-1] / (1-self.beta2 ** (epoch+1))\n",
        "            corrected_bias_nadam2 = self.biases_moment_nadam2[j-1] / (1-self.beta2 ** (epoch+1))\n",
        "\n",
        "            #netsrov momentum update\n",
        "            momentum_updated_weight = self.beta1*corrected_weight_nadam1 + ((1-self.beta1)*gradient_W) / (1 - self.beta1**(epoch+1))\n",
        "            momentum_updated_bias = self.beta1 * corrected_bias_nadam1 + ((1-self.beta1)*gradient_b) / (1 - self.beta1**(epoch+1))\n",
        "\n",
        "            #updating weights and biases\n",
        "            self.Weights[j-1] = self.Weights[j-1] - learning_rate_eta * momentum_updated_weight / (np.sqrt(corrected_weight_nadam2)+self.epsilon)\n",
        "            self.biases[j-1] = self.biases[j-1] - learning_rate_eta * momentum_updated_bias / (np.sqrt(corrected_bias_nadam2)+self.epsilon)\n",
        "\n",
        "        #computing average train accuracy\n",
        "        training_accuracy = accuracy / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training accuracy: {training_accuracy:.4f}')\n",
        "        wandb.log({'train-accuracy':training_accuracy*100})\n",
        "\n",
        "\n",
        "        #computing average epoch(training loss) loss\n",
        "        loss_epo = loss_epo / (x_train.shape[0] / batch_size)\n",
        "        print(f'Epoch Number {epoch+1}, training loss: {loss_epo:.4f}')\n",
        "        #self.train_loss_sgd.append(loss_epo)\n",
        "        wandb.log({'train-loss':loss_epo})\n",
        "\n",
        "        #computing accuracy on validation set\n",
        "        y_valid_predicted = self.forward_prop(x_valid)\n",
        "        valid_accuracy = accuracy_score(np.argmax(y_valid_predicted, axis=1), np.argmax(y_valid, axis=1))\n",
        "        print(f'Epoch Number {epoch+1}, validation accuracy: {valid_accuracy:.4f}')\n",
        "        wandb.log({'val_accuracy':valid_accuracy*100})\n",
        "        wandb.log({'epoch':epoch+1})\n",
        "\n",
        "        #computing validation loss\n",
        "        val_loss = loss_func(y_valid, y_valid_predicted)\n",
        "        print(f'Epoch Number {epoch+1}, validation loss: {val_loss:.4f}')\n",
        "        wandb.log({'val-loss':val_loss})\n",
        "\n",
        "      #checking the efficiency of the model by passing test set\n",
        "      y_test_predicted = self.forward_prop(x_test)\n",
        "      test_accuracy = accuracy_score(np.argmax(y_test_predicted, axis = 1), np.argmax(y_test, axis = 1))\n",
        "      print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "tBBez27xnvdW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKbpofe96z7S",
        "outputId": "579690e1-6251-4b79-c832-2c4f17199e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import random"
      ],
      "metadata": {
        "id": "JartcHi37RJg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='cd7a6c2259e8886dc269bbf6f0f9e55089d3beeb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMKdC7tf7fGq",
        "outputId": "28e35433-e642-4d2b-9496-777246edd89f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for best combination\n",
        "\n",
        "# You need to define a config file in the form of dictionary or yaml\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name' : 'best comb',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'hidden_layers':{\n",
        "            'values':[5]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values':[ 'adam']\n",
        "        },\n",
        "         'hidden_size':{\n",
        "            'values':[128]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values':[16]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-3]\n",
        "        },\n",
        "        'weight_init': {\n",
        "           'values' :['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['tanh']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Deep_Learning_Assignment1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCr6HFeAWWEL",
        "outputId": "378a5bd3-32c6-42ed-811c-7ab19749a53d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ehcdyzno\n",
            "Sweep URL: https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/ehcdyzno\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    '''\n",
        "    WandB calls main function each time with differnet combination.\n",
        "\n",
        "    We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    with wandb.init(entity = 'prabhat-kumar') as run:\n",
        "\n",
        "        run_name=\"-ac_\"+wandb.config.activation+\"-hs\"+str(wandb.config.hidden_size)+'-wi'+wandb.config.weight_init+'-hl'+str(wandb.config.hidden_layers)+'-op'+wandb.config.optimizer+'-ep'+str(wandb.config.epochs)+'lr'+str(wandb.config.learning_rate)+'bs'+str(wandb.config.batch_size) +'wd'+str(wandb.config.weight_decay)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "        model = Network([784,wandb.config.hidden_size,10], wandb.config.weight_init, wandb.config.activation, 0.9, 0.9, 0.999)\n",
        "\n",
        "        if wandb.config.optimizer == 'nesterov':\n",
        "          model.train_nag(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'momentum':\n",
        "          model.train_momentum(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'sgd':\n",
        "          model.train_sgd(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'adam':\n",
        "          model.train_adam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'rmsprop':\n",
        "          model.train_rmsprop(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'nadam':\n",
        "          model.train_nadam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52bc44a694104817930fe014eadd0449",
            "e8ca24f658de4ee5a2f50add4ffbd0fa",
            "303bb3d6e24f472088bcdcecb8783624",
            "48db94c8ba4746f1a7ae320638e1c9bb",
            "b00a83ab1bfa49ca804fb1f173a47ea9",
            "24bc7713031a437bbd4ce98ab12e0a38",
            "640969c4ef45467ea2515cfc0cfd1f82",
            "1c0901a2cf434372935553ec75e4c7b4"
          ]
        },
        "id": "0sf_JIRHW1ps",
        "outputId": "b7a4ca34-08cc-4994-da8a-1dbe947c6e44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9js8zqny with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240317_170411-9js8zqny</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/9js8zqny' target=\"_blank\">dazzling-sweep-3</a></strong> to <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/ehcdyzno' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/ehcdyzno</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/ehcdyzno' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/ehcdyzno</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/9js8zqny' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/9js8zqny</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number 1, training accuracy: 0.9181\n",
            "Epoch Number 1, training loss: 0.2836\n",
            "Epoch Number 1, validation accuracy: 0.9539\n",
            "Epoch Number 1, validation loss: 0.1706\n",
            "Epoch Number 2, training accuracy: 0.9550\n",
            "Epoch Number 2, training loss: 0.1558\n",
            "Epoch Number 2, validation accuracy: 0.9630\n",
            "Epoch Number 2, validation loss: 0.1391\n",
            "Epoch Number 3, training accuracy: 0.9641\n",
            "Epoch Number 3, training loss: 0.1258\n",
            "Epoch Number 3, validation accuracy: 0.9667\n",
            "Epoch Number 3, validation loss: 0.1229\n",
            "Epoch Number 4, training accuracy: 0.9700\n",
            "Epoch Number 4, training loss: 0.1067\n",
            "Epoch Number 4, validation accuracy: 0.9686\n",
            "Epoch Number 4, validation loss: 0.1126\n",
            "Epoch Number 5, training accuracy: 0.9741\n",
            "Epoch Number 5, training loss: 0.0929\n",
            "Epoch Number 5, validation accuracy: 0.9699\n",
            "Epoch Number 5, validation loss: 0.1053\n",
            "Epoch Number 6, training accuracy: 0.9776\n",
            "Epoch Number 6, training loss: 0.0821\n",
            "Epoch Number 6, validation accuracy: 0.9711\n",
            "Epoch Number 6, validation loss: 0.0998\n",
            "Epoch Number 7, training accuracy: 0.9804\n",
            "Epoch Number 7, training loss: 0.0733\n",
            "Epoch Number 7, validation accuracy: 0.9722\n",
            "Epoch Number 7, validation loss: 0.0954\n",
            "Epoch Number 8, training accuracy: 0.9827\n",
            "Epoch Number 8, training loss: 0.0658\n",
            "Epoch Number 8, validation accuracy: 0.9730\n",
            "Epoch Number 8, validation loss: 0.0919\n",
            "Epoch Number 9, training accuracy: 0.9849\n",
            "Epoch Number 9, training loss: 0.0594\n",
            "Epoch Number 9, validation accuracy: 0.9742\n",
            "Epoch Number 9, validation loss: 0.0890\n",
            "Epoch Number 10, training accuracy: 0.9865\n",
            "Epoch Number 10, training loss: 0.0537\n",
            "Epoch Number 10, validation accuracy: 0.9748\n",
            "Epoch Number 10, validation loss: 0.0866\n",
            "Test Accuracy: 0.9744\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52bc44a694104817930fe014eadd0449"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train-accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train-loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val-loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train-accuracy</td><td>98.646</td></tr><tr><td>train-loss</td><td>0.05374</td></tr><tr><td>val-loss</td><td>0.0866</td></tr><tr><td>val_accuracy</td><td>97.48</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-sweep-3</strong> at: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/9js8zqny' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/9js8zqny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240317_170411-9js8zqny/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for 2nd best combination\n",
        "\n",
        "\n",
        "# You need to define a config file in the form of dictionary or yaml\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name' : '2nd best comb',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'hidden_layers':{\n",
        "            'values':[4]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values':[ 'rmsprop']\n",
        "        },\n",
        "         'hidden_size':{\n",
        "            'values':[128]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values':[32]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-3]\n",
        "        },\n",
        "        'weight_init': {\n",
        "           'values' :['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['tanh']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Deep_Learning_Assignment1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBldjPDzYGud",
        "outputId": "40181354-1c5d-4779-d9f1-a88476769446"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: bp0k023t\n",
            "Sweep URL: https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/bp0k023t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    '''\n",
        "    WandB calls main function each time with differnet combination.\n",
        "\n",
        "    We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    with wandb.init(entity = 'prabhat-kumar') as run:\n",
        "\n",
        "        run_name=\"-ac_\"+wandb.config.activation+\"-hs\"+str(wandb.config.hidden_size)+'-wi'+wandb.config.weight_init+'-hl'+str(wandb.config.hidden_layers)+'-op'+wandb.config.optimizer+'-ep'+str(wandb.config.epochs)+'lr'+str(wandb.config.learning_rate)+'bs'+str(wandb.config.batch_size) +'wd'+str(wandb.config.weight_decay)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "        model = Network([784,wandb.config.hidden_size,10], wandb.config.weight_init, wandb.config.activation, 0.9, 0.9, 0.999)\n",
        "\n",
        "        if wandb.config.optimizer == 'nesterov':\n",
        "          model.train_nag(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'momentum':\n",
        "          model.train_momentum(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'sgd':\n",
        "          model.train_sgd(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'adam':\n",
        "          model.train_adam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'rmsprop':\n",
        "          model.train_rmsprop(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'nadam':\n",
        "          model.train_nadam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08e3f886ce4343be97670731489d05f6",
            "cf433c7bf4fe4d37835fd6dee6b90229",
            "42e56091ab9d4ee7bcf087db8748af91",
            "530638ffc2784ac095279d12ca43bb4f",
            "61a154224b524d2981bd5e22484bd0f6",
            "44259004640a44878b1e0825f512f05e",
            "1faa7c54a6224dca9448837da3de8b9e",
            "f6bfd8da7da7411b87e248ae04e1002d"
          ]
        },
        "id": "maqq30ZcZSNk",
        "outputId": "cc453c09-0703-4243-9a36-83b5b6b623ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5kdkukl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240317_171216-q5kdkukl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/q5kdkukl' target=\"_blank\">drawn-sweep-1</a></strong> to <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/bp0k023t' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/bp0k023t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/bp0k023t' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/bp0k023t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/q5kdkukl' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/q5kdkukl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number 1, training accuracy: 0.9361\n",
            "Epoch Number 1, training loss: 0.2241\n",
            "Epoch Number 1, validation accuracy: 0.9616\n",
            "Epoch Number 1, validation loss: 0.1391\n",
            "Epoch Number 2, training accuracy: 0.9696\n",
            "Epoch Number 2, training loss: 0.1092\n",
            "Epoch Number 2, validation accuracy: 0.9684\n",
            "Epoch Number 2, validation loss: 0.1110\n",
            "Epoch Number 3, training accuracy: 0.9795\n",
            "Epoch Number 3, training loss: 0.0778\n",
            "Epoch Number 3, validation accuracy: 0.9720\n",
            "Epoch Number 3, validation loss: 0.0976\n",
            "Epoch Number 4, training accuracy: 0.9854\n",
            "Epoch Number 4, training loss: 0.0575\n",
            "Epoch Number 4, validation accuracy: 0.9726\n",
            "Epoch Number 4, validation loss: 0.0893\n",
            "Epoch Number 5, training accuracy: 0.9904\n",
            "Epoch Number 5, training loss: 0.0428\n",
            "Epoch Number 5, validation accuracy: 0.9741\n",
            "Epoch Number 5, validation loss: 0.0833\n",
            "Epoch Number 6, training accuracy: 0.9939\n",
            "Epoch Number 6, training loss: 0.0314\n",
            "Epoch Number 6, validation accuracy: 0.9745\n",
            "Epoch Number 6, validation loss: 0.0803\n",
            "Epoch Number 7, training accuracy: 0.9967\n",
            "Epoch Number 7, training loss: 0.0227\n",
            "Epoch Number 7, validation accuracy: 0.9752\n",
            "Epoch Number 7, validation loss: 0.0794\n",
            "Epoch Number 8, training accuracy: 0.9980\n",
            "Epoch Number 8, training loss: 0.0161\n",
            "Epoch Number 8, validation accuracy: 0.9771\n",
            "Epoch Number 8, validation loss: 0.0785\n",
            "Epoch Number 9, training accuracy: 0.9992\n",
            "Epoch Number 9, training loss: 0.0111\n",
            "Epoch Number 9, validation accuracy: 0.9773\n",
            "Epoch Number 9, validation loss: 0.0802\n",
            "Epoch Number 10, training accuracy: 0.9998\n",
            "Epoch Number 10, training loss: 0.0076\n",
            "Epoch Number 10, validation accuracy: 0.9774\n",
            "Epoch Number 10, validation loss: 0.0831\n",
            "Test Accuracy: 0.9769\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08e3f886ce4343be97670731489d05f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train-accuracy</td><td>▁▅▆▆▇▇████</td></tr><tr><td>train-loss</td><td>█▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val-loss</td><td>█▅▃▂▂▁▁▁▁▂</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train-accuracy</td><td>99.984</td></tr><tr><td>train-loss</td><td>0.0076</td></tr><tr><td>val-loss</td><td>0.08313</td></tr><tr><td>val_accuracy</td><td>97.74</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">drawn-sweep-1</strong> at: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/q5kdkukl' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/q5kdkukl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240317_171216-q5kdkukl/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for 3rd best combination\n",
        "\n",
        "\n",
        "# You need to define a config file in the form of dictionary or yaml\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name' : '3rd best comb',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'hidden_layers':{\n",
        "            'values':[3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values':[ 'nadam']\n",
        "        },\n",
        "         'hidden_size':{\n",
        "            'values':[128]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values':[64]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-3]\n",
        "        },\n",
        "        'weight_init': {\n",
        "           'values' :['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['tanh']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Deep_Learning_Assignment1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHbOd7NgZn16",
        "outputId": "bd46ca33-7ca7-406d-8a4a-2c8be0b0dacb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: rt8vu9ze\n",
            "Sweep URL: https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/rt8vu9ze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    '''\n",
        "    WandB calls main function each time with differnet combination.\n",
        "\n",
        "    We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    with wandb.init(entity = 'prabhat-kumar') as run:\n",
        "\n",
        "        run_name=\"-ac_\"+wandb.config.activation+\"-hs\"+str(wandb.config.hidden_size)+'-wi'+wandb.config.weight_init+'-hl'+str(wandb.config.hidden_layers)+'-op'+wandb.config.optimizer+'-ep'+str(wandb.config.epochs)+'lr'+str(wandb.config.learning_rate)+'bs'+str(wandb.config.batch_size) +'wd'+str(wandb.config.weight_decay)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "        model = Network([784,wandb.config.hidden_size,10], wandb.config.weight_init, wandb.config.activation, 0.9, 0.9, 0.999)\n",
        "\n",
        "        if wandb.config.optimizer == 'nesterov':\n",
        "          model.train_nag(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'momentum':\n",
        "          model.train_momentum(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'sgd':\n",
        "          model.train_sgd(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'adam':\n",
        "          model.train_adam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'rmsprop':\n",
        "          model.train_rmsprop(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "        if wandb.config.optimizer == 'nadam':\n",
        "          model.train_nadam(x_train, y_train, wandb.config.learning_rate, wandb.config.epochs, wandb.config.batch_size)\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba378142800f4a1a81ae1fbcae7bb238",
            "008d7366a4ef4549b12d5a593b0a613a",
            "f19854e066cf4198b08663e5896cac0a",
            "4d50df139f6b4be09052cb24f3e83d00",
            "61182ee22fa2402b8d55193b15178b79",
            "7099f82f4b014bf2a704648a8b7fde62",
            "7b2d75bb315248d8bf830c021a372ae1",
            "999b569bcb50465fab19ad0911b4af64"
          ]
        },
        "id": "Q-gFLV2Ma9Q0",
        "outputId": "d147ab14-c7a7-4e53-fa35-8b0d88fbe381"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kcfu3f17 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240317_171826-kcfu3f17</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/kcfu3f17' target=\"_blank\">fiery-sweep-1</a></strong> to <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/q70g7bvn' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/q70g7bvn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/q70g7bvn' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/sweeps/q70g7bvn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/kcfu3f17' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/kcfu3f17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number 1, training accuracy: 0.9132\n",
            "Epoch Number 1, training loss: 0.2989\n",
            "Epoch Number 1, validation accuracy: 0.9542\n",
            "Epoch Number 1, validation loss: 0.1729\n",
            "Epoch Number 2, training accuracy: 0.9533\n",
            "Epoch Number 2, training loss: 0.1662\n",
            "Epoch Number 2, validation accuracy: 0.9612\n",
            "Epoch Number 2, validation loss: 0.1465\n",
            "Epoch Number 3, training accuracy: 0.9619\n",
            "Epoch Number 3, training loss: 0.1386\n",
            "Epoch Number 3, validation accuracy: 0.9638\n",
            "Epoch Number 3, validation loss: 0.1323\n",
            "Epoch Number 4, training accuracy: 0.9674\n",
            "Epoch Number 4, training loss: 0.1211\n",
            "Epoch Number 4, validation accuracy: 0.9653\n",
            "Epoch Number 4, validation loss: 0.1227\n",
            "Epoch Number 5, training accuracy: 0.9712\n",
            "Epoch Number 5, training loss: 0.1080\n",
            "Epoch Number 5, validation accuracy: 0.9672\n",
            "Epoch Number 5, validation loss: 0.1155\n",
            "Epoch Number 6, training accuracy: 0.9740\n",
            "Epoch Number 6, training loss: 0.0975\n",
            "Epoch Number 6, validation accuracy: 0.9683\n",
            "Epoch Number 6, validation loss: 0.1097\n",
            "Epoch Number 7, training accuracy: 0.9767\n",
            "Epoch Number 7, training loss: 0.0889\n",
            "Epoch Number 7, validation accuracy: 0.9701\n",
            "Epoch Number 7, validation loss: 0.1050\n",
            "Epoch Number 8, training accuracy: 0.9790\n",
            "Epoch Number 8, training loss: 0.0815\n",
            "Epoch Number 8, validation accuracy: 0.9706\n",
            "Epoch Number 8, validation loss: 0.1010\n",
            "Epoch Number 9, training accuracy: 0.9808\n",
            "Epoch Number 9, training loss: 0.0751\n",
            "Epoch Number 9, validation accuracy: 0.9723\n",
            "Epoch Number 9, validation loss: 0.0976\n",
            "Epoch Number 10, training accuracy: 0.9824\n",
            "Epoch Number 10, training loss: 0.0695\n",
            "Epoch Number 10, validation accuracy: 0.9728\n",
            "Epoch Number 10, validation loss: 0.0946\n",
            "Test Accuracy: 0.9699\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba378142800f4a1a81ae1fbcae7bb238"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train-accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train-loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val-loss</td><td>█▆▄▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train-accuracy</td><td>98.238</td></tr><tr><td>train-loss</td><td>0.06947</td></tr><tr><td>val-loss</td><td>0.09459</td></tr><tr><td>val_accuracy</td><td>97.28</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fiery-sweep-1</strong> at: <a href='https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/kcfu3f17' target=\"_blank\">https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment1/runs/kcfu3f17</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240317_171826-kcfu3f17/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}